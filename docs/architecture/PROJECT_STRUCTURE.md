# N8N Workflow Scraper - Project Structure v2.0

**Version:** 2.0 (Post-RND Feedback)  
**Date:** October 9, 2025  
**Status:** Optimized for 2-Week Sprint

---

## ğŸ“ **COMPLETE PROJECT STRUCTURE**

```
n8n-scraper/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                      # Project overview and quick start
â”œâ”€â”€ ğŸ“„ requirements.txt               # Python dependencies (27 packages)
â”œâ”€â”€ ğŸ“„ pyproject.toml                 # Python project configuration
â”œâ”€â”€ ğŸ“„ .env.example                   # Environment variables template
â”œâ”€â”€ ğŸ“„ .gitignore                     # Git ignore patterns
â”œâ”€â”€ ğŸ³ Dockerfile                     # Container definition
â”œâ”€â”€ ğŸ³ docker-compose.yml             # Docker orchestration
â”‚
â”œâ”€â”€ ğŸ“‚ src/                           # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ scraper/                   # Core scraping logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                   # Base scraper class
â”‚   â”‚   â”œâ”€â”€ page_extractor.py        # Layer 1: Page metadata
â”‚   â”‚   â”œâ”€â”€ workflow_extractor.py    # Layer 2: Workflow iframe
â”‚   â”‚   â”œâ”€â”€ explainer_extractor.py   # Layer 3: Explainer iframe
â”‚   â”‚   â””â”€â”€ orchestrator.py          # Main scraping orchestrator
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ processors/                # Data processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ocr.py                    # Image OCR processing
â”‚   â”‚   â”œâ”€â”€ video.py                  # Video transcript extraction
â”‚   â”‚   â”œâ”€â”€ validator.py              # Data validation
â”‚   â”‚   â””â”€â”€ transformer.py            # Data transformations
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ storage/                   # Data persistence
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database.py               # SQLite operations
â”‚   â”‚   â”œâ”€â”€ exporter.py               # Data export (JSON, JSONL, CSV)
â”‚   â”‚   â”œâ”€â”€ media.py                  # Media file management
â”‚   â”‚   â””â”€â”€ cache.py                  # Caching layer
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ models/                    # Data models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ schema.py                 # Pydantic models
â”‚   â”‚   â”œâ”€â”€ enums.py                  # Enumerations
â”‚   â”‚   â””â”€â”€ validators.py             # Custom validators
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ utils/                     # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py                 # Configuration management
â”‚       â”œâ”€â”€ logger.py                 # Logging setup
â”‚       â”œâ”€â”€ rate_limiter.py           # Rate limiting
â”‚       â””â”€â”€ retry.py                  # Retry logic
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                       # Executable scripts
â”‚   â”œâ”€â”€ scrape.py                     # Main scraping script
â”‚   â”œâ”€â”€ validate.py                   # Validation script
â”‚   â”œâ”€â”€ export.py                     # Export script
â”‚   â”œâ”€â”€ analyze.py                    # Dataset analysis
â”‚   â”œâ”€â”€ init_db.py                    # Database initialization
â”‚   â””â”€â”€ cleanup.py                    # Cleanup utilities
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                         # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                   # Pytest configuration
â”‚   â”œâ”€â”€ test_scraper.py               # Scraper tests
â”‚   â”œâ”€â”€ test_processors.py            # Processor tests
â”‚   â”œâ”€â”€ test_storage.py               # Storage tests
â”‚   â”œâ”€â”€ test_models.py                # Model validation tests
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ fixtures/                  # Test fixtures
â”‚       â”œâ”€â”€ sample_workflow.json      # Sample workflow data
â”‚       â”œâ”€â”€ sample_page.html          # Sample HTML page
â”‚       â””â”€â”€ sample_image.png          # Sample image
â”‚
â”œâ”€â”€ ğŸ“‚ config/                        # Configuration files
â”‚   â”œâ”€â”€ config.yaml                   # Main configuration
â”‚   â”œâ”€â”€ config.dev.yaml               # Development config
â”‚   â”œâ”€â”€ config.prod.yaml              # Production config
â”‚   â””â”€â”€ logging.yaml                  # Logging configuration
â”‚
â”œâ”€â”€ ğŸ“‚ data/                          # Data directory (gitignored)
â”‚   â”œâ”€â”€ ğŸ“‚ raw/                       # Raw scraped data
â”‚   â”‚   â””â”€â”€ workflows/                # Individual workflow JSON files
â”‚   â”œâ”€â”€ ğŸ“‚ processed/                 # Validated and processed data
â”‚   â”‚   â””â”€â”€ workflows/
â”‚   â”œâ”€â”€ ğŸ“‚ exports/                   # Final datasets
â”‚   â”‚   â”œâ”€â”€ dataset.json              # Complete JSON export
â”‚   â”‚   â”œâ”€â”€ dataset.jsonl             # Training-optimized
â”‚   â”‚   â”œâ”€â”€ dataset.csv               # Metadata summary
â”‚   â”‚   â””â”€â”€ dataset.parquet           # Columnar format
â”‚   â””â”€â”€ ğŸ“‚ database/                  # SQLite database
â”‚       â””â”€â”€ n8n_workflows.db
â”‚
â”œâ”€â”€ ğŸ“‚ media/                         # Media files (gitignored)
â”‚   â”œâ”€â”€ ğŸ“‚ images/                    # Downloaded images
â”‚   â”‚   â”œâ”€â”€ 2462_setup.png
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ğŸ“‚ videos/                    # Video metadata & transcripts
â”‚       â”œâ”€â”€ 2462_tutorial.json
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“‚ logs/                          # Log files (gitignored)
â”‚   â”œâ”€â”€ scraper.log                   # Main scraper logs
â”‚   â”œâ”€â”€ errors.log                    # Error logs
â”‚   â””â”€â”€ performance.log               # Performance metrics
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                          # Documentation
â”‚   â”œâ”€â”€ project-brief.md              # Project overview
â”‚   â”œâ”€â”€ technical-guide.md            # Implementation guide
â”‚   â”œâ”€â”€ api-docs.md                   # API documentation
â”‚   â”œâ”€â”€ dataset-schema.md             # Dataset structure
â”‚   â”œâ”€â”€ setup-guide.md                # Setup instructions
â”‚   â””â”€â”€ troubleshooting.md            # Common issues
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/                     # Jupyter notebooks (optional)
â”‚   â”œâ”€â”€ data-exploration.ipynb        # Dataset exploration
â”‚   â”œâ”€â”€ quality-analysis.ipynb        # Quality metrics
â”‚   â””â”€â”€ pattern-analysis.ipynb        # Workflow pattern analysis
â”‚
â””â”€â”€ ğŸ“‚ .github/                       # GitHub configuration (optional)
    â””â”€â”€ workflows/
        â”œâ”€â”€ test.yml                  # CI: Run tests
        â””â”€â”€ lint.yml                  # CI: Linting
```

---

## ğŸ“Š **DIRECTORY PURPOSES**

### **Source Code (`src/`)**

**`scraper/`** - Core scraping logic
- `base.py`: Abstract base scraper with common functionality
- `page_extractor.py`: Extract metadata from main page (Layer 1)
- `workflow_extractor.py`: Extract workflow JSON from iframe (Layer 2)
- `explainer_extractor.py`: Extract tutorials and media (Layer 3)
- `orchestrator.py`: Coordinates all extractors with rate limiting

**`processors/`** - Data processing
- `ocr.py`: Image text extraction using Tesseract
- `video.py`: Video transcript extraction
- `validator.py`: Pydantic-based data validation
- `transformer.py`: Data normalization and transformation

**`storage/`** - Data persistence
- `database.py`: SQLite operations with SQLAlchemy ORM
- `exporter.py`: Export to JSON, JSONL, CSV, Parquet
- `media.py`: Download and manage media files
- `cache.py`: Caching layer to avoid re-scraping

**`models/`** - Data structures
- `schema.py`: Pydantic models matching dataset schema
- `enums.py`: Enumerations (difficulty, categories, etc.)
- `validators.py`: Custom validation logic

**`utils/`** - Shared utilities
- `config.py`: Configuration management
- `logger.py`: Loguru setup
- `rate_limiter.py`: Rate limiting with aiolimiter
- `retry.py`: Retry logic with tenacity

---

### **Scripts (`scripts/`)**

**Main Scripts:**
- `scrape.py`: Main scraping orchestrator (CLI)
- `validate.py`: Validate scraped data quality
- `export.py`: Export dataset to various formats
- `analyze.py`: Generate quality reports

**Utility Scripts:**
- `init_db.py`: Initialize SQLite database
- `cleanup.py`: Clean up temp files and logs

---

### **Testing (`tests/`)**

**Test Structure:**
- Unit tests for each module
- Integration tests for scraping workflow
- Fixtures for consistent test data
- 80%+ code coverage target

**Key Test Files:**
- `test_scraper.py`: Test scraping logic
- `test_processors.py`: Test OCR, video processing
- `test_storage.py`: Test database and export
- `test_models.py`: Test Pydantic validation

---

### **Configuration (`config/`)**

**Config Files:**
- `config.yaml`: Main configuration (rates, limits, paths)
- `config.dev.yaml`: Development overrides
- `config.prod.yaml`: Production settings
- `logging.yaml`: Logging configuration

**Example Structure:**
```yaml
scraper:
  max_concurrent: 3
  rate_limit: 2  # requests per second
  timeout: 30
  headless: true

storage:
  images_dir: "./media/images"
  database: "sqlite:///data/database/n8n_workflows.db"

quality:
  min_completeness_score: 90
  require_explainer: true
```

---

### **Data (`data/`)**

**`raw/`** - Unprocessed scraped data
- Individual workflow JSON files
- Timestamped for tracking

**`processed/`** - Validated data
- Validated workflows meeting quality criteria
- Ready for training

**`exports/`** - Final datasets
- Multiple formats for different use cases
- Versioned exports

**`database/`** - SQLite database
- Queryable workflow data
- Supports complex queries

---

### **Media (`media/`)**

**`images/`** - Downloaded images
- Original images from workflows
- Named by workflow ID + sequence
- OCR text extracted and stored in database

**`videos/`** - Video metadata
- YouTube video metadata
- Transcripts in JSON format
- Links to original videos

---

## ğŸš€ **QUICK START COMMANDS**

### **Local Development**

```bash
# Setup
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
playwright install chromium
cp .env.example .env

# Initialize database
python scripts/init_db.py

# Run scraper (10 workflows test)
python scripts/scrape.py --workflows 10

# Validate data
python scripts/validate.py

# Export dataset
python scripts/export.py --format jsonl

# Run tests
pytest tests/ -v --cov=src
```

### **Docker Development**

```bash
# Build image
docker-compose build

# Run scraper
docker-compose up scraper

# Run with database viewer
docker-compose --profile dev up

# Run specific task
docker-compose run --rm scraper scripts/scrape.py --workflows 100

# Export data
docker-compose run --rm scraper scripts/export.py --format json

# Stop all
docker-compose down
```

---

## ğŸ“¦ **FILE SIZE ESTIMATES**

```
Source Code:        ~50 KB
Dependencies:       ~400 MB (27 packages)
Data (2,100 workflows):
  - Raw JSON:       ~50 MB
  - Database:       ~75 MB
  - Media:          ~2-5 GB (images + videos)
  - Exports:        ~100 MB (all formats)
Total Project:      ~3-5 GB
```

---

## ğŸ¯ **DEVELOPMENT WORKFLOW**

### **Day 1: Setup**
1. Clone repository
2. Install dependencies
3. Setup Docker environment
4. Initialize database
5. Test with 5 workflows

### **Days 2-5: Core Development**
1. Implement page extractor
2. Implement workflow extractor
3. Implement explainer extractor
4. Add OCR and video processing
5. Write unit tests

### **Days 6-10: Integration**
1. Build orchestrator
2. Add rate limiting
3. Implement retry logic
4. Add validation
5. Integration tests

### **Days 11-14: Polish**
1. Export functionality
2. Quality analysis
3. Documentation
4. Production testing
5. Dataset delivery

---

## âœ… **SUCCESS CHECKLIST**

Before considering the project complete:

- [ ] All 27 dependencies installed
- [ ] Playwright browser installed
- [ ] Database initialized
- [ ] Can scrape 1 workflow successfully
- [ ] Can scrape 10 workflows with 95%+ success
- [ ] All 3 layers extracting data
- [ ] OCR processing images
- [ ] Video transcripts extracted
- [ ] Data validation passing
- [ ] Export to all formats working
- [ ] Tests passing (80%+ coverage)
- [ ] Docker setup working
- [ ] Documentation complete

---

**This structure is optimized for:**
- âœ… Clear separation of concerns
- âœ… Easy testing and maintenance
- âœ… Scalable architecture
- âœ… Professional development practices
- âœ… 2-week sprint timeline

**Ready for implementation!** ğŸš€