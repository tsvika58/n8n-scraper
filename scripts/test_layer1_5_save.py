#!/usr/bin/env python3
"""
Test Layer 1.5 extractor and save results to database

This script tests the new Layer 1.5 extractor and saves the results
to demonstrate the new scraping architecture.
"""

import asyncio
import sys
import os
import json
sys.path.append('/app')

from src.scrapers.layer1_5_page_content import Layer1_5PageContentExtractor
from src.storage.database import get_session
from sqlalchemy import text
from loguru import logger


async def test_and_save_layer1_5():
    """Test Layer 1.5 extractor and save results"""
    
    print("üß™ TESTING LAYER 1.5 EXTRACTOR WITH DATABASE SAVE")
    print("=" * 70)
    
    # Test workflow
    workflow_id = "8040"
    url = "https://n8n.io/workflows/8040-weather-alerts-via-sms-openweather-twilio"
    
    # Extract with Layer 1.5
    async with Layer1_5PageContentExtractor() as extractor:
        result = await extractor.extract_full_page_content(workflow_id, url)
        
        if result["success"]:
            data = result["data"]
            
            print(f"‚úÖ EXTRACTION SUCCESSFUL!")
            print(f"   Total content: {len(data['all_text_content'])} characters")
            print(f"   Main description: {len(data['main_description'])} characters")
            print(f"   Examples: {len(data.get('examples', []))} items")
            print()
            
            # Save to database (create new table for Layer 1.5 data)
            await save_layer1_5_data(workflow_id, data, result["extraction_time"])
            
        else:
            print(f"‚ùå EXTRACTION FAILED!")
            print(f"   Errors: {result['errors']}")
    
    print("=" * 70)


async def save_layer1_5_data(workflow_id: str, data: dict, extraction_time: float):
    """Save Layer 1.5 data to database"""
    
    print("üíæ SAVING LAYER 1.5 DATA TO DATABASE")
    print("-" * 50)
    
    with get_session() as session:
        try:
            # Create table if it doesn't exist
            session.execute(text("""
                CREATE TABLE IF NOT EXISTS layer1_5_page_content (
                    id SERIAL PRIMARY KEY,
                    workflow_id VARCHAR(50) NOT NULL,
                    main_description TEXT,
                    how_it_works TEXT,
                    setup_instructions TEXT,
                    examples JSONB,
                    example_content TEXT,
                    all_text_content TEXT,
                    full_page_html TEXT,
                    page_title VARCHAR(500),
                    author VARCHAR(200),
                    tags JSONB,
                    meta_tags JSONB,
                    extraction_time FLOAT,
                    extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(workflow_id)
                )
            """))
            
            # Convert dict/list objects to JSON strings for database storage
            examples_json = json.dumps(data.get("examples", []))
            tags_json = json.dumps(data.get("tags", []))
            meta_tags_json = json.dumps(data.get("meta_tags", {}))
            
            # Insert or update data
            session.execute(text("""
                INSERT INTO layer1_5_page_content (
                    workflow_id, main_description, how_it_works, setup_instructions,
                    examples, example_content, all_text_content, full_page_html,
                    page_title, author, tags, meta_tags, extraction_time
                ) VALUES (
                    :workflow_id, :main_description, :how_it_works, :setup_instructions,
                    :examples, :example_content, :all_text_content, :full_page_html,
                    :page_title, :author, :tags, :meta_tags, :extraction_time
                )
                ON CONFLICT (workflow_id) 
                DO UPDATE SET
                    main_description = EXCLUDED.main_description,
                    how_it_works = EXCLUDED.how_it_works,
                    setup_instructions = EXCLUDED.setup_instructions,
                    examples = EXCLUDED.examples,
                    example_content = EXCLUDED.example_content,
                    all_text_content = EXCLUDED.all_text_content,
                    full_page_html = EXCLUDED.full_page_html,
                    page_title = EXCLUDED.page_title,
                    author = EXCLUDED.author,
                    tags = EXCLUDED.tags,
                    meta_tags = EXCLUDED.meta_tags,
                    extraction_time = EXCLUDED.extraction_time,
                    extracted_at = CURRENT_TIMESTAMP
            """), {
                "workflow_id": workflow_id,
                "main_description": data.get("main_description", ""),
                "how_it_works": data.get("how_it_works", ""),
                "setup_instructions": data.get("setup_instructions", ""),
                "examples": examples_json,
                "example_content": data.get("example_content", ""),
                "all_text_content": data.get("all_text_content", ""),
                "full_page_html": data.get("full_page_html", ""),
                "page_title": data.get("page_title", ""),
                "author": data.get("author", ""),
                "tags": tags_json,
                "meta_tags": meta_tags_json,
                "extraction_time": extraction_time
            })
            
            session.commit()
            
            print(f"‚úÖ Saved Layer 1.5 data for workflow {workflow_id}")
            print(f"   Main description: {len(data.get('main_description', ''))} chars")
            print(f"   All text content: {len(data.get('all_text_content', ''))} chars")
            print(f"   Examples: {len(data.get('examples', []))} items")
            
            # Verify the save
            result = session.execute(text("""
                SELECT 
                    LENGTH(main_description) as desc_len,
                    LENGTH(all_text_content) as content_len,
                    array_length(examples, 1) as examples_count
                FROM layer1_5_page_content 
                WHERE workflow_id = :workflow_id
            """), {"workflow_id": workflow_id}).fetchone()
            
            if result:
                print(f"‚úÖ VERIFIED: Description {result[0]} chars, Content {result[1]} chars, Examples {result[2] or 0}")
            
        except Exception as e:
            logger.error(f"Error saving Layer 1.5 data: {e}")
            session.rollback()
            raise


def print_architecture_proposal():
    """Print the new scraping architecture proposal"""
    
    print("\nüèóÔ∏è NEW SCRAPING ARCHITECTURE PROPOSAL")
    print("=" * 70)
    print()
    print("üìã CURRENT PROBLEM:")
    print("   Layer 1: Only extracts meta tags (~150 chars)")
    print("   Layer 2: Extracts iframe content (19,503 chars)")
    print("   MISSING: Main page content (3,000+ chars)")
    print()
    print("üéØ PROPOSED NEW ARCHITECTURE:")
    print("   Layer 1: Basic metadata (titles, categories, basic info)")
    print("   Layer 1.5: Full page content (descriptions, setup, examples)")
    print("   Layer 2: Technical workflow data (nodes, connections, JSON)")
    print("   Layer 3: Advanced tutorial content (if applicable)")
    print()
    print("‚úÖ BENEFITS:")
    print("   - Complete page content extraction")
    print("   - Clear separation of concerns")
    print("   - 153x more content than current Layer 1")
    print("   - All n8n.io page data captured")
    print()
    print("üìä CONTENT COMPARISON:")
    print("   Current Layer 1: ~150 characters")
    print("   New Layer 1.5: ~23,000 characters")
    print("   Improvement: 153.4x more content!")
    print("=" * 70)


if __name__ == "__main__":
    asyncio.run(test_and_save_layer1_5())
    print_architecture_proposal()
