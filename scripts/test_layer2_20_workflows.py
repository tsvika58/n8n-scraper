#!/usr/bin/env python3
"""
Test Layer 2 Enhanced on 20 workflows with full monitoring.
Validates resume mechanism and database updates.
"""

import asyncio
import sys
import os
import psycopg2
import json
from datetime import datetime, timedelta
from time import time
from loguru import logger

sys.path.insert(0, '/Users/tsvikavagman/Desktop/Code Projects/shared-tools/n8n-scraper')

from src.scrapers.layer2_enhanced import EnhancedLayer2Extractor

# Configure logger (clean format like Layer 1)
logger.remove()
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
    level="INFO"
)

# Database credentials
DB_HOST = "aws-1-eu-north-1.pooler.supabase.com"
DB_PORT = "5432"
DB_NAME = "postgres"
DB_USER = "postgres.skduopoakfeaurttcaip"
DB_PASSWORD = "crg3pjm8ych4ctu@KXT"


class Layer2TestRunner:
    """Test runner for 20 workflows."""
    
    def __init__(self):
        self.start_time = None
        self.processed = 0
        self.successful = 0
        self.failed = 0
        self.total_workflows = 20
        self.extraction_times = []
        self.errors = []
    
    def get_test_workflows(self):
        """Get 20 workflows for testing."""
        
        try:
            conn = psycopg2.connect(
                host=DB_HOST,
                port=DB_PORT,
                database=DB_NAME,
                user=DB_USER,
                password=DB_PASSWORD,
                sslmode='require'
            )
            
            cursor = conn.cursor()
            
            # Get 20 workflows that need Layer 2
            cursor.execute("""
                SELECT w.workflow_id, w.url
                FROM workflows w
                WHERE (w.layer2_success IS NULL OR w.layer2_success = false)
                ORDER BY w.workflow_id::integer ASC
                LIMIT 20;
            """)
            
            workflows = cursor.fetchall()
            
            cursor.close()
            conn.close()
            
            return workflows
            
        except Exception as e:
            logger.error(f"Error fetching workflows: {e}")
            return []
    
    def store_to_database(self, workflow_id, result):
        """Store extraction result to Supabase."""
        
        try:
            conn = psycopg2.connect(
                host=DB_HOST,
                port=DB_PORT,
                database=DB_NAME,
                user=DB_USER,
                password=DB_PASSWORD,
                sslmode='require'
            )
            
            cursor = conn.cursor()
            
            # Prepare data
            api_data = result['sources']['api']
            iframe_data = result['sources']['iframe']
            
            # Extract node types
            nodes = api_data.get('data', {}).get('workflow', {}).get('nodes', [])
            node_types = list(set(node.get('type') for node in nodes if node.get('type')))
            
            # Insert/Update workflow_structure
            cursor.execute("""
                INSERT INTO workflow_structure (
                    workflow_id, node_count, connection_count, node_types,
                    extraction_type, fallback_used, workflow_json,
                    iframe_data, visual_layout, enhanced_content, media_content,
                    extraction_sources, completeness_metrics, extracted_at
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (workflow_id) DO UPDATE SET
                    node_count = EXCLUDED.node_count,
                    connection_count = EXCLUDED.connection_count,
                    node_types = EXCLUDED.node_types,
                    extraction_type = EXCLUDED.extraction_type,
                    fallback_used = EXCLUDED.fallback_used,
                    workflow_json = EXCLUDED.workflow_json,
                    iframe_data = EXCLUDED.iframe_data,
                    visual_layout = EXCLUDED.visual_layout,
                    enhanced_content = EXCLUDED.enhanced_content,
                    media_content = EXCLUDED.media_content,
                    extraction_sources = EXCLUDED.extraction_sources,
                    completeness_metrics = EXCLUDED.completeness_metrics,
                    extracted_at = EXCLUDED.extracted_at;
            """, (
                workflow_id,
                api_data.get('node_count'),
                api_data.get('connection_count'),
                json.dumps(node_types),
                api_data.get('extraction_type', 'full'),
                api_data.get('fallback_used', False),
                json.dumps(api_data.get('data')),
                json.dumps(iframe_data.get('nodes')),
                json.dumps(iframe_data.get('visual_layout')),
                json.dumps(iframe_data.get('enhanced_content')),
                json.dumps(iframe_data.get('media_content')),
                json.dumps({'api': api_data.get('success'), 'iframe': iframe_data.get('success')}),
                json.dumps(result.get('completeness')),
                datetime.utcnow()
            ))
            
            # Update layer2_success flag in workflows table
            cursor.execute("""
                UPDATE workflows 
                SET layer2_success = true, 
                    extracted_at = %s
                WHERE workflow_id = %s
            """, (datetime.utcnow(), workflow_id))
            
            conn.commit()
            cursor.close()
            conn.close()
            
            logger.info(f"      ‚úÖ Stored to Supabase and updated layer2_success flag")
            return True
            
        except Exception as e:
            logger.error(f"      ‚ùå Storage error: {e}")
            return False
    
    def print_progress(self):
        """Print progress dashboard."""
        
        elapsed = time() - self.start_time
        progress_pct = (self.processed / self.total_workflows) * 100
        
        # Calculate ETA
        if self.processed > 0:
            avg_time = elapsed / self.processed
            remaining = self.total_workflows - self.processed
            eta_seconds = remaining * avg_time
            eta_minutes = int(eta_seconds // 60)
            eta_seconds = int(eta_seconds % 60)
            eta = f"{eta_minutes}m {eta_seconds}s"
        else:
            eta = "calculating..."
        
        # Progress bar
        bar_length = 50
        filled = int(bar_length * self.processed / self.total_workflows)
        bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)
        
        # Format elapsed time
        elapsed_minutes = int(elapsed // 60)
        elapsed_seconds = int(elapsed % 60)
        elapsed_str = f"{elapsed_minutes}m {elapsed_seconds}s"
        
        # Average extraction time
        avg_extraction = sum(self.extraction_times) / len(self.extraction_times) if self.extraction_times else 0
        
        logger.info("="*80)
        logger.info(f"üìä PROGRESS: {self.processed}/{self.total_workflows} ({progress_pct:.1f}%)")
        logger.info(f"[{bar}]")
        logger.info(f"‚úÖ Success: {self.successful} | ‚ùå Failed: {self.failed}")
        logger.info(f"‚è±Ô∏è  Elapsed: {elapsed_str} | ETA: {eta}")
        logger.info(f"‚ö° Speed: {avg_extraction:.1f}s per workflow")
        logger.info("="*80)
    
    async def process_workflow(self, extractor, workflow_id, url, index):
        """Process a single workflow."""
        
        logger.info("")
        logger.info(f"[{index}/{self.total_workflows}] Processing workflow #{workflow_id}")
        logger.info(f"   URL: {url}")
        
        try:
            # Extract
            result = await extractor.extract_complete(workflow_id, url)
            
            logger.info(f"   ‚úÖ Extraction complete: {result['extraction_time']:.2f}s")
            logger.info(f"   üìä Completeness: {result['completeness']['merged']:.1f}%")
            
            # Store
            stored = self.store_to_database(workflow_id, result)
            
            # Track
            self.processed += 1
            if result['completeness']['merged'] == 100.0 and stored:
                self.successful += 1
                self.extraction_times.append(result['extraction_time'])
            else:
                self.failed += 1
                self.errors.append({
                    'workflow_id': workflow_id,
                    'error': 'Storage failed' if not stored else 'Incomplete extraction'
                })
            
            # Print progress after EVERY workflow
            self.print_progress()
            
            return True
            
        except Exception as e:
            self.processed += 1
            self.failed += 1
            self.errors.append({
                'workflow_id': workflow_id,
                'error': str(e)
            })
            logger.error(f"   ‚ùå Error: {e}")
            return False
    
    async def run(self):
        """Run test on 20 workflows."""
        
        logger.info("="*80)
        logger.info("üß™ LAYER 2 ENHANCED - TEST RUN (20 WORKFLOWS)")
        logger.info("="*80)
        logger.info(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("")
        
        # Get workflows
        logger.info("üìä Fetching 20 workflows for testing...")
        workflows = self.get_test_workflows()
        
        if not workflows:
            logger.warning("‚ö†Ô∏è  No workflows found to test!")
            return
        
        logger.info(f"‚úÖ Found {len(workflows)} workflows to test")
        logger.info("")
        logger.info("="*80)
        logger.info("üîÑ STARTING TEST EXTRACTION")
        logger.info("="*80)
        
        self.total_workflows = len(workflows)
        self.start_time = time()
        
        # Process workflows
        async with EnhancedLayer2Extractor() as extractor:
            for i, (workflow_id, url) in enumerate(workflows, 1):
                await self.process_workflow(extractor, workflow_id, url, i)
                
                # Rate limiting
                if i < len(workflows):
                    await asyncio.sleep(2)
        
        # Final summary
        self.print_final_summary()
    
    def print_final_summary(self):
        """Print final summary."""
        
        elapsed = time() - self.start_time
        elapsed_minutes = int(elapsed // 60)
        elapsed_seconds = int(elapsed % 60)
        elapsed_str = f"{elapsed_minutes}m {elapsed_seconds}s"
        
        logger.info("")
        logger.info("")
        logger.info("="*80)
        logger.info("üìä TEST SUMMARY")
        logger.info("="*80)
        logger.info("")
        logger.info(f"Total Workflows: {self.total_workflows}")
        logger.info(f"‚úÖ Successful: {self.successful} ({self.successful/self.total_workflows*100:.1f}%)")
        logger.info(f"‚ùå Failed: {self.failed} ({self.failed/self.total_workflows*100:.1f}%)")
        
        if self.extraction_times:
            avg_time = sum(self.extraction_times) / len(self.extraction_times)
            min_time = min(self.extraction_times)
            max_time = max(self.extraction_times)
            
            logger.info("")
            logger.info("‚è±Ô∏è  Extraction Times:")
            logger.info(f"   Average: {avg_time:.2f}s")
            logger.info(f"   Fastest: {min_time:.2f}s")
            logger.info(f"   Slowest: {max_time:.2f}s")
        
        logger.info("")
        logger.info(f"‚è±Ô∏è  Total Time: {elapsed_str}")
        
        if self.errors:
            logger.info("")
            logger.info(f"‚ö†Ô∏è  Errors ({len(self.errors)}):")
            for error in self.errors[:5]:
                logger.info(f"   ‚Ä¢ Workflow {error['workflow_id']}: {error['error']}")
        
        logger.info("")
        logger.info(f"Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("")
        logger.info("‚úÖ Test complete! Resume mechanism validated.")
        logger.info("")


async def main():
    """Main entry point."""
    runner = Layer2TestRunner()
    await runner.run()


if __name__ == "__main__":
    asyncio.run(main())

